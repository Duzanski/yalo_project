# Yalo Take-Home Challege 

Table of Contents
=================
 * [Project Tree](#project_tree)
 * [Task #1](#task_1)
 * [Task #2](#task_2)

## Project_Tree
```python
       ðŸ“¦.dbt
        â”£ ðŸ“‚models
        â”ƒ â”£ ðŸ“‚reports
        â”ƒ â”ƒ â”£ ðŸ“œreport_category_more_revenue copy.sql
        â”ƒ â”ƒ â”— ðŸ“œreport_stores_more_revenue.sql
        â”ƒ â”£ ðŸ“‚sources
        â”ƒ â”ƒ â”— ðŸ“œsources.yml
        â”ƒ â”— ðŸ“‚transform
        â”ƒ â”ƒ â”£ ðŸ“œdim_category.sql
        â”ƒ â”ƒ â”£ ðŸ“œdim_date.sql
        â”ƒ â”ƒ â”£ ðŸ“œdim_item.sql
        â”ƒ â”ƒ â”£ ðŸ“œdim_store.sql
        â”ƒ â”ƒ â”£ ðŸ“œdim_vendor.sql
        â”ƒ â”ƒ â”— ðŸ“œfact_sales.sql
        â”£ ðŸ“œdbt_project.yml
        â”£ ðŸ“œpackages.yml
        â”— ðŸ“œprofiles.yml
        ðŸ“¦dags
        â”£ ðŸ“œ.airflowignore
        â”— ðŸ“œsales.py
        ðŸ“¦include
        â”£ ðŸ“‚dataset
        â”ƒ â”— ðŸ“œsales.csv
        â”£ ðŸ“‚gcp
        â”ƒ â”— ðŸ“œservice_account.json
        â”— ðŸ“‚soda
        â”ƒ â”£ ðŸ“‚checks
        â”ƒ â”ƒ â”£ ðŸ“‚sources
        â”ƒ â”ƒ â”ƒ â”— ðŸ“œraw_sales.yml
        â”ƒ â”ƒ â”— ðŸ“‚transform
        â”ƒ â”ƒ â”ƒ â”£ ðŸ“œdim_category.yml
        â”ƒ â”ƒ â”ƒ â”£ ðŸ“œdim_date.yml
        â”ƒ â”ƒ â”ƒ â”£ ðŸ“œdim_item.yml
        â”ƒ â”ƒ â”ƒ â”£ ðŸ“œdim_store.yml
        â”ƒ â”ƒ â”ƒ â”£ ðŸ“œdim_vendor.yml
        â”ƒ â”ƒ â”ƒ â”— ðŸ“œfact_sales.yml
        â”ƒ â”£ ðŸ“œcheck_function.py
        â”ƒ â”— ðŸ“œconfiguration.yml
        â”ƒðŸ“œDockerfile
        â”ƒðŸ“œrequirements.txt
```

## Task_1
* Calculate the total products and revenue sold over time by quarter and
identify the month where the revenue sold was 10% above the average.
```sql
  WITH monthly_revenue AS (
  SELECT 
    EXTRACT(MONTH FROM date) AS month,
    EXTRACT(QUARTER FROM date) AS quarter,
    EXTRACT(YEAR FROM date) AS year,
    COUNT(DISTINCT item_number) AS total_products,
    SUM(sale_dollars) AS total_revenue
  FROM `bigquery-public-data.iowa_liquor_sales.sales`
  GROUP BY year, quarter, month
),
average_revenue AS (
  SELECT AVG(total_revenue) AS avg_revenue
  FROM monthly_revenue
)
SELECT 
  month, 
  year, 
  quarter, 
  total_products, 
  total_revenue,
  CASE 
    WHEN total_revenue > 1.1 * avg_revenue THEN 'Above Average'
    ELSE 'Below Average'
  END AS Revenue_Comparison
FROM monthly_revenue, average_revenue
ORDER BY year, quarter, month
```

* List the counties where the amount (in dollars) of purchases transactions went over $100K.
```sql
SELECT county
FROM `bigquery-public-data.iowa_liquor_sales.sales`
GROUP BY county
HAVING SUM(sale_dollars) > 100000;
```

* Identify the top 10 stores with more revenue in sold products and the bottom
stores with least revenue in sold products (apply a deduplication logic in case
itâ€™s needed).
```sql
-- Top 10 stores with the most revenue
SELECT store_name, SUM(sale_dollars) as total_revenue
FROM `bigquery-public-data.iowa_liquor_sales.sales`
GROUP BY store_name
ORDER BY total_revenue DESC
LIMIT 10;

-- Bottom 10 stores with the least revenue
SELECT store_name, SUM(sale_dollars) as total_revenue
FROM `bigquery-public-data.iowa_liquor_sales.sales`
GROUP BY store_name
ORDER BY total_revenue ASC
LIMIT 10;
```

## Task_2

* Please provide a data lineage of the data pipeline and design appropriate data layers
for this case. Briefly describe what is the underlying logic of every layer and why you
chose it.

        âž¥ It's important to mention that, when data modeling, it's crucial to be closely aligned with the business team to truly understand their needs. In the context of this test, I took a broader perspective. However, in a real-world scenario, we should always focus on modeling with the aim of identifying the business processes, granularity, dimensions, and measures.
    * To solve this task I first analysed the table an modeled the data using the start schema pattern as image below.
![Alt text](image.png)
    * Afterward, I created a project using Dbt, employed SODA for data validation, utilized Airflow, and integrated Google BigQuery as the data warehouse for working on the data pipeline.
     
        âžœ sdsdsda
    ![Alt text](image-1.png)
    * Below you can find the grid of the DAG
    ![Alt text](image-2.png)
    * As a result, you can find the analytics tables under the following path.
        ```python
        ðŸ“¦.dbt
        â”£ ðŸ“‚models
        â”ƒ â”£ ðŸ“‚reports
        â”ƒ â”ƒ â”£ ðŸ“œreport_category_more_revenue copy.sql
        â”ƒ â”ƒ â”— ðŸ“œreport_stores_more_revenue.sql
        ```

  